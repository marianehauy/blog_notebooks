{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teorema de Bayes\n",
    "\n",
    "Olá, pessoal! Este é o primeiro artigo de uma série sobre uma classe especial de classificadores que temos em aprendizado de máquina: os **classificadores bayesianos**. Eles recebem esse nome pois todos são derivados do famoso **Teorema de Bayes**. Essa série será composta por 4 artigos: neste primeiro vamos comentar sobre o Teorema de Bayes propriamente dito; no segundo, vamos ver como o teorema pode ser utilizado no contexto de aprendizado de máquina; no terceiro e quarto daremos alguns exemplos de algoritmos que o utilizam como base teórica, como o *Naive Bayes* e as análises de discriminante linear (*LDA*, do inglês *Linear Discriminant Analysis*) e quadrática (*QDA*, do inglês *Quadratic Discriminant Analysis*). Sem mais delongas, vamos direto ao assunto deste artigo!\n",
    "\n",
    "### Um exemplo com dados (piada do ano)\n",
    "\n",
    "Para essa série, vamos assumir que o leitor esteja familiarizado com conceitos de probabilidade, mas vamos dar uma breve revisada em alguns. Para tornar o raciocínio mais fácil de acompanhar vamos utilizar um exemplo bem simples de dados. Suponha que estamos interessados em calcular algumas probabilidades referente ao lançamento de um dado, honesto (importante frizar), de seis faces. Por exemplo, podemos querer a probabilidade de que o número da face virada para cima seja par ou que seja exatamento o número 2. No nosso dado, temos 6 possibilidades de saída: os números de 1 a 6. Como supusemos que o dado era honesto, todas as face tem a mesma probabilidade de serem sorteadas, ou seja,\n",
    "\n",
    "$$P(i) = \\frac{1}{6}, \\ \\forall\\ i \\in {1,...,6}$$\n",
    "\n",
    "\n",
    "De maneira geral, para este exemplo simples, vamos dizer que as nossas probabilidade serão sempre a quantidade de elementos que eu tenho no meu evento de interesse sobre o total de possibilidade, que será sempre 6. Pode ter ficado um pouco confuso, mas, como sempre, os exemplos vão nos ajudar. Demos anteriormente o exemplo de que queríamos a probabilidade de o número sorteado ser par. Pensando nas possibilidades, percebemos que isso significa que a face deve estar presente no conjunto $\\{2,4,6\\}$. Desse modo, o nosso evento de interesse tem 3 elementos. Logo,\n",
    "\n",
    "$$P(\\{2,4,6\\}) = \\frac{3}{6} = 0.5$$\n",
    "\n",
    "Da mesma forma, o segundo exemplo nos diz que o nosso evento de interesse é somente a face 2 e temos somente um elemento. Assim,\n",
    "\n",
    "$$P(2) = \\frac{1}{6}$$\n",
    "\n",
    "### Probabilidade condicional\n",
    "\n",
    "Essas probabilidades que estimamos são referentes a um cenário em que a única informação que temos é que o dado é honesto. Mas e se com o passar do tempo eu fosse descobrindo mais informações sobre os dados. Por exemplo, suponha alguém peça para você adivinhar qual o valor da face virada para cima. Inicialmente, como o dado é honesto e todas as probabilidade são iguais, chutar qualquer um dos 6 valores é o melhor que podemos fazer. Porém, se alguém me disser que o número sorteado é par, já não faz mais sentido, por exemplo, eu chutar os números 1, 3 e 5. Nesse novo cenário, as minhas probabilidades foram atualizadas por uma informação extra que me foi dada. \n",
    "\n",
    "Tendo em mente esse cenário, vamos tentar definir o que seria uma probabilidade condicional. Suponha que temos dois eventos A e B, a probabilidade de o evento A acontecer dado que sabemos que o evento B aconteceu é dada por \n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "na qual $P(B)$ é a probabilidade do evento B acontecer e $P(A \\cap B)$ é a probabilidade da intersecção entre os eventos A e B. Como tudo é mais fácil de entender com um exemplo, vamos lá. Vamos supor que $\\text{A} = \\{2,4,6\\}$, ou seja, a face sorteada é par e $\\text{B} = \\{4,5,6\\}$, logo, a face sorteada é maior que 3. Podemos ver que a intersecção entre A e B é $A \\cap B = \\{4,6\\}$. Pela nossa regra simples de probabilidade, temos que \n",
    "\n",
    "$$P(B) = \\frac{3}{6} = \\frac{1}{2}$$\n",
    "\n",
    "e \n",
    "\n",
    "$$P(A \\cap B) = \\frac{2}{6} = \\frac{1}{3}$$\n",
    "\n",
    "Logo, utilizando a definição de probabilidade condicional, podemos calcular a probabilidade de A dado que o evento B aconteceu: \n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{\\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}$$\n",
    "\n",
    "Que legal! Agora temos um número resultante de uma fórmula esquisita, mas o que raios isso quer dizer??? \n",
    "\n",
    "Vamos por partes e colocar as coisas em perspectiva. Vamos dizer que você está em uma aposta com um amigo: você está apostando que a face virada para cima é par e seu amigo aposta que é ímpar. Inicialmente, a única coisa que sabemos é que a face virada pode ser qualquer número entre 1 e 6.\n",
    "\n",
    "<div>\n",
    "    <center> \n",
    "        <img src=\"images/bayes_01.png\" width=\"600px\" height=\"500px\"/> \n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "Nesse cenário, a probabilidade de que você vença a aposta é de 50%: existem 3 números que são pares dos 6 possíveis e 3 ímpares. Abaixo temos uma representação visual do evento A.\n",
    "\n",
    "<div>\n",
    "    <center> \n",
    "        <img src=\"images/bayes_02.png\" width=\"600px\" height=\"500px\"/> \n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "E se agora, uma terceira pessoa tivesse olhado o dado e tivesse dito para vocês dois que o evento B ocorreu, ou seja, o número virado para cima é maior que 3. Você aumentaria o valor que está apostando? Em outras palavras, dado que você sabe que o número é 4, 5 ou 6, você acredita que sua probabilidade de vencer aumentou?\n",
    "\n",
    "<div>\n",
    "    <center> \n",
    "        <img src=\"images/bayes_03.png\" width=\"600px\" height=\"500px\"/> \n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "Vamos tentar olhar de uma maneira um pouco diferente para responder essa pergunta. Nós sabemos com certeza que o número sorteado é 4, 5 ou 6. Isso significa que a probabilidade de os números 1, 2 e 3 terem sido sorteados é zero, ou seja, o nosso espaço de possibilidades foi reduzido para os elementos do evento B. Mas, dentro desse espaço, não temos mais 6 possibilidades e sim 3. Também temos apenas 2 números pares em vez de 3. Dessa forma, dado que o evento B ocorreu, a probabilidade do evento A mudou para\n",
    "\n",
    "$$P(A|B) = \\frac{2}{3}$$\n",
    "\n",
    "<div>\n",
    "    <center> \n",
    "        <img src=\"images/bayes_04.png\" width=\"600px\" height=\"500px\"/> \n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "De maneira sucinta, uma probabilidade condicional indica que, sob a luz de novas observações, a nossa crença inicial deve ser alterada. Dado que temos novas informações, nosso espaço de possibilidades é menor e isso pode aumentar ou diminuir a probabilidade de algum outro evento de interesse. Essa mudança na nossa crença\n",
    "\n",
    "### O famoso teorema\n",
    "\n",
    "Bem legais esses conceitos, né? O maior problema dessas fórmulas é que, na prática, as probabilidades conjuntas são muito difíceis de serem calculadas. Vamos brincar um pouco com as fórmulas e ver se conseguimos um jeito de eliminá-las da equação. Vamos usar de novo nossos eventos A e B, e escrever duas equações:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "e \n",
    "\n",
    "$$P(B|A) = \\frac{P(A \\cap B)}{P(A)}$$\n",
    "\n",
    "Perceba que as duas equações possuem o termo $P(A \\cap B)$. Fazendo uma pequena manipulação, podemos isolá-lo em ambas. Essa equação também é conhecida como **regra da multiplicação**:\n",
    "\n",
    "$$P(A \\cap B) = P(A|B)P(B)$$\n",
    "\n",
    "e \n",
    "\n",
    "$$P(A \\cap B) = P(B|A)P(A)$$\n",
    "\n",
    "Você já deve ter percebido pra onde isso está indo. Adivinha qual o próximo passo? Exatamente, vamos igualar as duas equações:\n",
    "\n",
    "$$P(A|B)P(B) = P(B|A)P(A)$$\n",
    "\n",
    "Logo,\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "E esse é o famoso **Teorema de Bayes**? SIM! Essa é a forma mais simples do teorema. Mas nós podemos incrementá-la um pouco mais. Mas nada tema, já chegamos lá. Ainda falta um conceito importante para completarmos o teorema: **partição**. Formalmente (ou quase isso), podemos definí-la como:\n",
    "\n",
    "> Uma partição é um conjunto de eventos $P_i,\\ \\forall i=1,...,n$ que satisfaz as seguintes condições:\n",
    "> 1. $P_i \\cap P_j = \\emptyset,\\ \\forall\\ i,j\\ \\text{with}\\ i \\neq j$\n",
    "> 2. $\\sum_{i=1}^n P(P_i) = 1$\n",
    "\n",
    "\"Lá vem ele de novo com essas definições esquisitas e sem sentido\", você deve estar pensando. Porém, como sempre, nada é tão assustador quanto parece à primeira vista. A primeira condição quer dizer que não existe interseção nenhuma entre dois eventos, ou seja, os eventos são **disjuntos**. A segunda basicamente quem dizer que a união de todos os eventos tem que cobrir todas as possíveis saídas. Só assim é que a soma das probabilidade dará 1. Viu? Nem foi tão difícil.\n",
    "\n",
    "Vamos voltar ao nosso exemplo do dado. Existem inúmeras formas que podemos dividir o espaço, mas vamos definir uma muito simples, só para efeitos didáticos: $P_1$ vão ser os números de 1 a 3 e $P_2$ de 4 a 5.\n",
    "\n",
    "<div>\n",
    "    <center> \n",
    "        <img src=\"images/bayes_05.png\" width=\"600px\" height=\"500px\"/> \n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "Perceba que, como a partição cobre todas as saídas possíveis, posdemos escrever qualquer evento $E$ que quisermos da seguinte maneira:\n",
    "\n",
    "$$E = \\bigcup_{i=1}^n E \\cap P_i = (E \\cap P_1) \\cup (E \\cap P_2)$$\n",
    "\n",
    "Ahn??? Confuso né! Mas vamos ver o que isso significa quando colocamos o nosso evento A (o número ser par). $A \\cap P_1$ significa todos os elementos que estão tanto em $A$ quanto em $P_1$. No nosso caso, isso é referente ao conjunto $\\{2\\}$. O mesmo pode ser aplicado à $A \\cap P_2$, e chegamos ao conjunto $\\{4,6\\}$. Logo,\n",
    "\n",
    "$$(A \\cap P_1) \\cup (A \\cap P_2) = \\{2\\} \\cup \\{4,6\\} = \\{2,4,6\\} = A$$\n",
    "\n",
    "<div>\n",
    "    <center> \n",
    "        <img src=\"images/bayes_06.png\" width=\"600px\" height=\"500px\"/> \n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "UAU! Não é que realmente deu certo? Mas será que dá certo com outros eventos também? Vamos testar com o evento B para tirar essa dúvida: $B \\cap P_1 = \\emptyset$ e $B \\cap P_2 = \\{4,5,6\\}$. Portanto,  \n",
    "\n",
    "$$(B \\cap P_1) \\cup (B \\cap P_2) = \\emptyset \\cup \\{4,5,6\\} = \\{4,5,6\\} = B$$\n",
    "\n",
    "<div>\n",
    "    <center> \n",
    "        <img src=\"images/bayes_07.png\" width=\"600px\" height=\"500px\"/> \n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "De novo! Que incrível! E fica ainda melhor! Vamos olhar para o probabilidade de um evento, baseado nessa fórmula:\n",
    "\n",
    "$$P(E) = \\sum_{i=1}^n P(E \\cap P_i)$$\n",
    "\n",
    "Mas, pela regra da multiplicação, $P(E \\cap P_i) = P(E|P_i)P(P_i)$. Desse modo,\n",
    "\n",
    "$$P(E) = \\sum_{i=1}^n P(E|P_i)P(P_i)$$\n",
    "\n",
    "Na equação original do teorema, podemos substituir nosso evento A, por alguma partição $P_j$ e isso nos levaria à:\n",
    "\n",
    "$$P(P_j|B) = \\frac{P(B|P_j)P(P_j)}{P(B)}$$\n",
    "\n",
    "Mas acabamos de ver que $P(B) = \\sum_{i=1}^n P(B|P_i)P(P_i)$. Logo,\n",
    "\n",
    "$$P(P_j|B) = \\frac{P(B|P_j)P(P_j)}{\\sum_{i=1}^n P(B|P_i)P(P_i)}$$\n",
    "\n",
    "E essa é a forma extendida do teorema! \n",
    "\n",
    "Ufa! Achou que nunca ia terminar né. Conseguimos chegar até aqui e para finalizar vamos dar um exemplo de como esse teorema pode ser útil na prática.\n",
    "\n",
    "### Um exemplo (quase) de verdade\n",
    "\n",
    "Vamos supor que exista uma doença que atinja 1% da população. Existe um teste para detectar a doença mas ele não é perfeito: em 95% das pessoas com a doença o teste dá positivo e em 2% das pessoas que não tem a doença o teste também da positivo (erroneamente). Selecionando uma pessoa aleatoriamente, se o teste der positivo, qual a probabilidade de essa pessoa ter essa doença. Bom, vamos começar do começo, não é mesmo? Vamos chamar $D$ o evento da pessoa ter a doença e $\\overline{D}$ o evento da pessoa não ter a doença. Assim,\n",
    "\n",
    "$$P(D) = 0,01$$\n",
    "\n",
    "e\n",
    "\n",
    "$$P(\\overline{D}) = 0,99$$\n",
    "\n",
    "Legal! Se analisarmos bem, $D$ e $\\overline{D}$ formam uma partição. Vamos olhar agora as outras informações que temos: se $T$ é o evento de o teste dar positivo, então:\n",
    "\n",
    "$$P(T|D) = 0,95$$\n",
    "\n",
    "e \n",
    "\n",
    "$$P(T|\\overline{D}) = 0,02$$\n",
    "\n",
    "Estamos interessados em calcular $P(D|T)$ então vamos usar o nosso querido teorema:\n",
    "\n",
    "$$P(D|T) = \\frac{P(T|D)P(D)}{P(T)}$$\n",
    "\n",
    "Se analisarmos o denominador, temos todas as informações que precisamos. O problema está no denominador. Mas isso realmente não é um problema, não é mesmo? Temos exatamente a solução para nossos problemas:\n",
    "\n",
    "$$P(T) = P(T|D)P(D) + P(T|\\overline{D})P(\\overline{D})$$\n",
    "\n",
    "$$P(T) = 0,95*0,01 + 0,02*0,99 = 0,0293$$\n",
    "\n",
    "Muito bom! Agora já temos todos os elementos que precisamos! Então vamos lá:\n",
    "\n",
    "$$P(D|T) = \\frac{0,95*0,01}{0,0293} = \\frac{0,0095}{0,0293} = 0,32$$\n",
    "\n",
    "Ou seja, dado que o exame deu positivo, temos apenas uma probabilidade de 32% de que essa pessoa realmente tenha a doença. Bem pequena, não é?\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "Neste primeiro artigo da série discutimos um pouco sobre o famoso teorema de Bayes: suas equações, interpretações e até um exemplo. Essa é uma ferramente muito útil quando estamos falando de dados e ter um bom entendimento do conceito pode ajudar em vários contextos. No próximo artigo, vamos ver uma aplicação do mesmo teorema quando pensamos em aprendizado de máquina. Não perca!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
