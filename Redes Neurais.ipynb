{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Redes Neurais</h1>\n",
    "</center>\n",
    "\n",
    "<p>Fala, pessoal! Hoje vamos falar um pouco sobre o que talvez seja o modelo mais comentado atualmente quando falamos de Inteligência Artificial: as famosas <b>Redes Neurais</b>. Apesar de bastante conhecidas, muitos acreditam que seu funcionamento é muito complexo para ser entendido e preferem acreditar que se trata de uma \"caixa preta\" ou \"mágica\". O objetivo dessa série de artigos é mostrar como realmente esse algoritmo funciona e que não é nenhum \"ser sobrenatural\".</p>\n",
    "\n",
    "Nesse primeiro artigo, vamos estudar quais elementos compõem um rede neural e como podemos utilizá-los para realizar previsões.\n",
    "\n",
    "<center>\n",
    "    <h2>O que são Redes Neurais?</h2>\n",
    "</center>\n",
    "\n",
    "<p>\n",
    "Antes de mais nada vamos começar dando uma visão geral dos componentes de uma Rede Neural:\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_01.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "<p>\n",
    "A imagem acima mostra uma arquitetura genérica para redes neurais na qual podemos ver claramente 3 estruturas:\n",
    "    <ol>\n",
    "        <li>\n",
    "            Camada de entrada (em azul): essa é estrutura responsável por receber as informações que queremos utilizar como variáveis explicativas do nosso modelo;\n",
    "        </li>\n",
    "        <li>\n",
    "            Camada de saída (em verde): estrutura final da rede, responsável por dar a previsão final em que estamos interessados;\n",
    "        </li>\n",
    "        <li>\n",
    "            Camadas ocultas (em laranja): são representações intermediárias que auxiliam na passagem do nosso conjunto de dados de entrada para a nossa saída. São chamadas ocultas pois as saídas dessas camadas não são vistas pelos usuários finais. \n",
    "        </li>\n",
    "    </ol>\n",
    "</p>\n",
    "\n",
    "As camadas são as estruturas macro da rede. Mas do que elas são compostas?\n",
    "\n",
    "O elemento básico de uma camada é chamado de neurônio (por isso o algoritmo se chama redes neurais). Um neurônio nada mais é do que uma combinação linear (o que??) de suas entradas seguida pela aplicação de uma função de ativação. Falamos vários termos técnicos, mas o que realmente isso quer dizer?\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_02.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "<h3>Combinação Linear</h3>\n",
    "\n",
    "Uma combinação linear significa que vamos multiplicar cada valor de entrada $X_i$ por um outro valor $w_i$, denominado <b>peso</b>, e depois somar todos os resultados dessa multiplicação. Além de somarmos os valores de entrada multiplicados pelos seus pesos, também somamos um valor constante:\n",
    "\n",
    "$$\\large c(X_1, ..., X_n) = b\\ +\\ w_1X_1\\ +\\ w_2X_2\\ +\\ ...\\ +\\ w_nX_n$$\n",
    "\n",
    "Mas espere um pouco, essa não é uma equação conhecida? Exatamente: esse é a equação do algoritmo de <a href=\"https://medium.com/@lauradamaceno/regress%C3%A3o-linear-6a7f247c3e29\">Regressão Linear</a>!!! Uma regressão linear nada mais é do que uma combinação linear das variáveis de entrada. E uma regressão linear também pode ser vista como uma rede neural muito simples: não temos camadas escondidas, a camada de saída tem somente um neurônio e a função de ativação dele é a função identidade $f(z) = z$.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_03.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Vamos usar um exemplo prático. Suponha que queremos prever o peso de uma pessoa sabendo sua altura. Nossos dados estão dispostos como abaixo:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_04.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Nessa caso, temos uma única variável explicativa, a altura, e a nossa variável alvo é o peso. Assim nossa rede neural assume a seguinte forma:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_05.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Vamos supor também que nosso modelo já foi treinado (mostraremos em outro post como esse treino é realizado) e que obtemos os seguintes parâmetros: $w_1 = 0.43$ e $b=12$. Vamos utilizar o nosso modelo para realizar uma previsão sobre o peso de uma pessoa com 177 cm de altura. Para fazer isso, basta multiplicarmos a nossa altura, 177 cm, por 0.43 e somarmos 12, resultando em 88.1. Também precisamos passar esse resultado pela função de ativação, que no caso é a identidade e então nós temos que a previsão final é 88.1 kg.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_06.gif\" width=\"800px\" height=\"400px\">]\n",
    "</center>\n",
    "\n",
    "Muito simples não é mesmo, mas vamos adicionar um pouco mais de complexidade aqui.\n",
    "\n",
    "<h3>Funções de ativação</h3>\n",
    "\n",
    "Quando analisamos os dados do nosso problema, podemos perceber que existe uma relação linear entre a altura e o peso. O nosso modelo anterior, com um único neurônio e função de ativação identidade tem justamente o formato de uma reta (ou um hiperplano em problemas com mais variáveis), como podemos ver abaixo.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_07.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Mas e se por acaso tivemos um outro problema, dessa vez de classificação, que se comporta da seguinte maneira:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_08.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Agora estamos tentando prever se uma pessoa compraria uma casa em determinada região baseado na sua renda. Perceba que nossa variável alvo é limitada agora entre 0 (não comprou) e 1 (comprou). Vamos ver como nosso modelo anterior funcionaria.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_09.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Não ficou muito legal, não é? Alguns valores de previsão ficaram abaixo de 0 e outros acima de 1 (o que isso significa???). Para o nosso problema esses valores não tem sentido algum, mas somente com uma combinação linear dos nosso valores de entrada, nós não temos como controlar quais os valores limites que a previsão pode assumir. É ai que entra o primeiro motivo para usarmos uma função de ativação:\n",
    "\n",
    "> <b>Na camada de saída, a função de ativação é utilizada para mapearmos a saída da combinação linear dos neurônios dessa camada para  o domínio esperado</b>\n",
    "\n",
    "Como nossos valores esperados estão entre 0 e 1, podemos por exemplo utilizar uma função chamada <b>sigmóide</b> para mapear a saída. A sigmóide tem a seguinte equação:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Vamos analisá-la com calma agora: quando x é muito grande, vamos dizer $\\infty$, $e^{-x}$ tende a 0 e, assim, $\\sigma(x)$ tende a 1. Em contrapartida, se x é muito pequeno, como $-\\infty$, $e^{-x}$ tende a $\\infty$ e como consequência $\\sigma(x)$ tende a 0. Desse modo, conseguimos limitar a nossa saída para o domínio desejado. Mas espere! Essa função também já não é conhecida? Novamente você está certo: quando aplicamos a sigmóide na saída da combinação linear temos a mesma equação da <a href=\"https://matheusfacure.github.io/2017/02/25/regr-log/\">Regressão Logística<a>.\n",
    "\n",
    "Vamos observar isso funcionaria no nosso diagrama da rede. Novamente vamos supor que ela já está treinada (vamos chegar lá, mais um pouco de paciência). Dessa vez, vamos tentar prever se um pessoa com renda de 4500 reais compraria uma casa nessa região.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_10.gif\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Podemos perceber que mesmo a saída da combinação linear sendo maior do que 1, a saída final do modelo é menor, como esperávamos que fosse. Esse comportamento se mantém para os outros de renda também. Agora não temos mais o problema de ter previsões que não fazem sentido!\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_11.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "<h2>Adicionando camadas</h2>\n",
    "\n",
    "Vimos até agora uma arquitura básica de redes neurais, na qual não existia nenhuma camada oculta. Entretanto o grande poder das redes neurais vem das camadas ocultas. Já vamos entender o porquê disso, mas antes vamos introduzir algumas notações:\n",
    "\n",
    "- $n_{ij}$: neurônio *j* da camada oculta *i*;\n",
    "- $x_{ik}$: entrada *k* dos neurônios da camada oculta *i*;\n",
    "- $o_{ij}$: saída do $n_{ij}$;\n",
    "- $w_{ij}^k$: peso da entrada *k* para o neurônio $n_{ij}$;\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_12.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Utilizando essa notação, vamos escrever as equações da nossa rede na forma matricial (é só uma forma diferente de escrever que vai facilitar nossa vida daqui pra frente).\n",
    "\n",
    "- <b>Entrada da camada oculta *i*</b>: \n",
    "$$\n",
    "X_i = \n",
    "\\begin{bmatrix}\n",
    "x_{i1}\\\\\n",
    "\\vdots\\\\\n",
    "x_{ik}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{k\\times 1}\n",
    "$$\n",
    "\n",
    "- <b>Saída da camada oculta *i*</b>: \n",
    "$$\n",
    "O_i = \n",
    "\\begin{bmatrix}\n",
    "o_{i1}\\\\\n",
    "\\vdots\\\\\n",
    "o_{ij}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{j\\times 1}\n",
    "$$\n",
    "\n",
    "- <b>Pesos da camada oculta *i*</b>: \n",
    "$$\n",
    "W_i = \n",
    "\\begin{bmatrix}\n",
    "w_{i1}^1 & \\cdots & w_{ij}^1 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "w_{i1}^k& \\cdots & w_{ij}^k\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{k \\times j}\n",
    "$$\n",
    "\n",
    "Assim, a equação que mapeia a entrada $X_i$ para a saída $O_i$ é:\n",
    "\n",
    "$$O_i = f(W_i^T X_i + b_i),$$\n",
    "\n",
    "em que $b_i \\in \\mathbb{R}^{j\\times 1}$ é um vetor com os *bias* de cada neurônio da camada.\n",
    "\n",
    "<h3>O poder das funções de ativação</h3>\n",
    "\n",
    "Na camada de saída já vimos que a função de ativação é importante para limitarmos o domínio da saída para coincidir com o domínio esperado. Mas existe uma outra utilidade que talvez seja a mais importante. Vamos supor que no nosso exemplo com duas camada ocultas e dois neurônios em cada não utilizamos nenhuma função de ativação. \n",
    "\n",
    "Na primeira camada temos:\n",
    "$$O_1 = W_1^T X_1+ b_1$$\n",
    "\n",
    "Já na segunda camada, a saída da camada anterior é a nossa entrada, então:\n",
    "\n",
    "$$O_2 = W_2^T O_1+ b_2 $$\n",
    "\n",
    "$$O_2 = W_2^T (W_1^T X_1 + b_1)+ b_2$$\n",
    "\n",
    "$$O_2 = (W_2^T W_1^T) X_1 + (W_2^T b_1 + b_2)$$\n",
    "\n",
    "$$O_2 = W^{\\prime T}_2 X_1 + b^\\prime_2$$\n",
    "\n",
    "Calma ai! O que é esse monte de equação?? Podem ficar tranquilos, apesar de parecer complicado o que queremos mostrar é muito simples: se não utilizarmos uma função de ativação, a saídas das camadas mais profundas serão apenas combinações lineares da entrada. É como se estivessemos fazendo cálculos extras para chegar na mesma reposta. Tendo isso em mente, a segunda característica importante das funções de ativação é:\n",
    "\n",
    "> <b>A funções de ativação introduzem não-linearidades nas camadas intermediárias que diminuem o viés do modelo como um todo. Caso elas não existissem, só conseguiríamos modelar, até a camada de saída, funções lineares.</b>\n",
    "\n",
    "<h2>Ensinando Redes Neurais</h2>\n",
    "\n",
    "Até o momento, sempre que passávamos por uma iteração da rede dizíamos que os pesos já estavam treinados, mas nunca comentamos como os pesos foram treinados. Chegou a hora! Vamos falar de como podemos ajustar os pesos para que tenhamos previsões cada vez melhores.\n",
    "\n",
    "<h3>O Gradiente</h3>\n",
    "Para introduzirmos esse assunto, precisamos lembrar de um conceito de cálculo: o <b>gradiente</b>. Em termos matemáticos, podemos escrever o gradiente da seguinte maneira:\n",
    "\n",
    "$$\n",
    "\\nabla f(x_1, \\cdots, x_n) =  \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial f}{\\partial x_n}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n\\times 1}\n",
    "$$\n",
    "\n",
    "mas isso não quer dizer muita coisa. Para entendermos como ele é utilizado para ensinar as redes, precisamos entender um pouco de sua intuição geométrica.\n",
    "\n",
    "> O vetor gradiente de uma função $f$ indica a direção em que o valor da função cresce de maneira mais rápida.\n",
    "\n",
    "Ok ... ainda não entendi. Sem problemas, vamos utilizar um exemplo mais prático. Suponha que queremos encontrar o valor máximo da função $f(x) = -x^2$ ($f$ só depende de uma variável, mas para efeitos didáticos fica mais fácil de ensinar). Graficamente, ela tem o seguinte formato.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_13.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Vamos supor também que já estamos em cima dessa curva, no ponto $x=-4$. Como $f$ tem dependência somente na variável $x$, temos duas opções: ou adicionamos um valor positivo em $x$ e \"andamos\" para a direita, ou adicionamos um valor negativo e vamos para a esquerda.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_14.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Visualmente, podemos ver que devemos ir para a direita, mas como podemos ter certeza pensando matematicamente? É ai que entra o gradiente! Como falamos o gradiente indica a direção de maior crescimento da função. Vamos começar calculando então o vetor gradiente.\n",
    "\n",
    "$$\\nabla f(x) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-2x\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Vamos com calma agora. Dissemos que o vetor nos daria uma direção, mas pelo que calculamos ainda temos uma função de $x$. Isso é devido ao fato de para cada $x$ termos uma valor diferente de gradiente o que leva a uma direção diferente também. Para sabermos qual a direção no ponto em que estamos, basta substituirmos o valor do ponto atual no gradiente. Assim,\n",
    "\n",
    "$$\n",
    "\\nabla f(-4) = \n",
    "\\begin{bmatrix}\n",
    "-2x\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "8\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Ótimo! Calculamos o nosso gradiente, vimos que ele é positivo e então temos que \"andar\" para a direita, como esperávamos. Mas quanto devemos andar? Vamos tentar dar um passo da magnitude do gradiente $|\\nabla f(-4)| = 8$. Com isso vamos parar em $x=4$.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_15.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Hum... Não parece que melhorou muito. Ainda estamos com um valor $f(4)=16$, o mesmo que na posição anterior. E se calcularmos o gradiente de novo? \n",
    "\n",
    "$$\n",
    "\\nabla f(4) = \n",
    "\\begin{bmatrix}\n",
    "-2x\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-8\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Dessa vez o gradiente é negativo, logo temos que ir para a esquerda. Porém se dermos de novo um \"passo\" da magnitude do vetor gradiente, voltaríamos para $x=-4$, a nossa posição inicial. Parece que esse método não está levando a nenhum lugar, não é? Vamos tentar então uma outra abordagem. Ao invés da darmos um passo com a magnitude total do gradiente, vamos utilizar só uma porcentagem desse valor. Para testar, vamos fazer $10\\%$.\n",
    "\n",
    "$$x^\\prime = x + 0.1*|\\nabla f(x)| = -4 + 0.1 * 8 = -3.2$$\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_16.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Uou! Dessa vez conseguimos aumentar o valor da nossa função para $f(-3.2) = -10.24$. Parece promissor, mas vamos tentar mais uma vez.\n",
    "\n",
    "$$x^\\prime = x + 0.1*|\\nabla f(x)| = -3.2 + 0.1* 6.4 = -2.56$$\n",
    "\n",
    "Ainda melhor! Agora temos $f(-2.56) = -6.55$.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_17.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Parece que essa abordagem de utilizar uma porcentagem da magnitude do gradiente funcionou muito bem e é exatamente essa técnica que vamos utilizar. Na verdade, essa porcentagem que utilizamos tem um nome muito especial: <b>taxa de aprendizagem</b> e a referenciamos pela letra grega $\\alpha$. É ela quem controla o tamanho do passo que damos. Pudemos ver que se o passo for muito longo, como quando utilizamos o valor 1 ($100\\%$), podemos nunca aumentar o valor ou, pior ainda, diminuí-lo. Mas e se a taxa de aprendizagem for muito pequena? Nesse caso não temos problema com a convergência, mas sim o tempo que leva para a atingirmos. Vamos supor que agora $\\alpha = 0.01$ ou seja $1\\%$.\n",
    "\n",
    "$$x_1 = x_0 + 0.01*|\\nabla f(x_0)| = -4 + 0.01*8 = -3.92$$\n",
    "\n",
    "$$x_2 = x_1 + 0.01*|\\nabla f(x_1)| = -3.92 + 0.01* 8 = -3.84$$\n",
    "\n",
    "Veja que depois de duas iterações, atingimos o valor de $f(-3.84) = -14.75$, enquanto antes tinhamos $f(-2.56) = -6.55$. Podemos ver que a escolha da taxa de aprendizagem não é trivial:\n",
    "\n",
    "> Se $\\alpha$ for muito grande corremos o risco de não convergir para o máximo e se for muito pequena podemos demorar muito para atingí-lo.\n",
    "\n",
    "<h3>Função de perda</h3>\n",
    "\n",
    "A utilização do gradiente é ótima para otimizarmos funções, mas que função nós estamos querendo otimizar quando falamos de redes neurais? Para tangibilizar, vamos tratar especificamente da rede mais simples vimos até agora: a regressão linear. Vamos até utilizar nosso exemplo anterior de previsão do peso em função da altura.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_18.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Inicialmente nem nosso parâmetro $w$ nem nosso parâmetro $b$ estão treinados. Geralmente, eles são iniciados com um valor aletório, vamos dizer $w=0.7$ e $b=6$. A curva de previsão da nossa rede é então:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_19.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Não está muito legal, não é? Mas quão ruim ela está? Vamos tomar o exemplo de um ponto específico Altura = 177cm (você já viu esse ponto em algum lugar?). Vamos observar como é dada a previsão para esse ponto.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_20.gif\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Vamos supor que o valor correto para essa altura é de 88.2. Sendo assim, o quão distante nossa previsão de 129.9 está do valor real? Para responder a essa pergunta precisamo definir uma função de perda (*loss functions*). Para mantermos os cálculos simples, vamos utilizar uma função muito comum: o <b>erro quadrático</b>.\n",
    "\n",
    "$$\\mathscr{L}(y, ŷ) = \\frac{1}{2} (ŷ-y)^2,$$\n",
    "\n",
    "em que $y$ é o valor observado e $ŷ$ é o valor previsto. Um ponto interessante dessa função é que queremos minimizá-la ao invés de maximizá-la como fizemos anteriormente, afinal queremos ter uma diferença cada vez menor entre a resposta real e o valor previsto. Isso significa que não podemos utilizar o gradiente? De jeito nenhum! Ainda podemos (e vamos) utilizá-lo mas com uma pequena modificação:\n",
    "\n",
    "> Quando queremos minimizar uma função não utilizamos a direção do gradiente e sim a direção inversa. Isso nos indica em qual direção a função tende a diminuir mais rápido.\n",
    "\n",
    "Podemos pensar que quando queremos maximar uma função estamos escalando-a na direção do gradiente e quando queremos minimizá-la estamos descendo. É por esse motivo que chamamos esse algoritmo de <b>gradiente descendente</b>. \n",
    "\n",
    "Vamos tentar agora atualizar nossos parâmetros. No nosso modelo temos 2 parâmetros que precisam ser treinados: $w$ e $b$. Vamos começar com $w$:\n",
    "\n",
    "$$ŷ = w * x + b$$\n",
    "\n",
    "Logo,\n",
    "\n",
    "$$\\frac{\\partial \\mathscr{L}}{\\partial w} = \\frac{\\partial \\mathscr{L}}{\\partial ŷ} \\frac{\\partial ŷ}{\\partial w}$$\n",
    "\n",
    "Aqui utilizamos outro conceito de cálculo: a <b><a href=\"https://pt.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction\">regra da cadeia</a></b>. Vamos calcular essas duas derivadas separadamente:\n",
    "\n",
    "$$\\frac{\\partial \\mathscr{L}}{\\partial ŷ} = ŷ-y$$\n",
    "\n",
    "e \n",
    "\n",
    "$$\\frac{\\partial ŷ}{\\partial w} = x$$\n",
    "\n",
    "Com isso, podemos combiná-las e chegaremos a:\n",
    "\n",
    "$$\\frac{\\partial \\mathscr{L}}{\\partial w} = (ŷ-y)*x = (88.2-129.9)*177 = 7380.9$$\n",
    "\n",
    "Vamos então atualizar nosso peso $w$. A mudança que temos que fazer agora é que para utilizar a direção inversa do gradiente, em vez de somá-lo ao valor atual, vamos subtraí-lo.\n",
    "\n",
    "$$w^\\prime = w - \\alpha * \\frac{\\partial \\mathscr{L}}{\\partial w}$$\n",
    "\n",
    "Vamos utilizar $\\alpha=0.00005$ (como chegamos nesse valor é outro caso).\n",
    "\n",
    "$$w^\\prime = 0.7 - 0.00005 * 7380.9 = 0.7 - 0.36 = 0.34$$\n",
    "\n",
    "Legal! Já temos nosso novo valor para $w$. Se fizermos o mesmo procedimento para $b$, chegaremos na seguinte equação:\n",
    "\n",
    "$$\\frac{\\partial \\mathscr{L}}{\\partial b} = (ŷ-y) = 129.9 - 88.2 = 41.7$$\n",
    "\n",
    "Para atualizarmos nosso parâmetro:\n",
    "\n",
    "$$b^\\prime = b - \\alpha * \\frac{\\partial \\mathscr{L}}{\\partial b}$$\n",
    "\n",
    "$$b^\\prime = 6 - 0.00005 * 41.7 = 6 - 0.002 = 5.998$$\n",
    "\n",
    "Ótimo! Agora temos nosso dois novos parâmetros. Vamos ver qual a nova previsão para esse modelo.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_21.gif\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Mas será que o erro diminuiu mesmo? Vamos verificar.\n",
    "\n",
    "$$\\mathscr{L} (88.2, 129.9) = \\frac{1}{2} (129.9-88.2)^2 = \\frac{1}{2} 41.7^2 = 869.45$$\n",
    "\n",
    "$$\\mathscr{L} (88.2, 66.2) = \\frac{1}{2} (66.2-88.2)^2 = \\frac{1}{2} 22^2 = 242$$\n",
    "\n",
    "Uau! Diminuimos muito o erro!\n",
    "\n",
    "<h3>Função de custo</h3>\n",
    "\n",
    "Um ponto importante da discussão anterior é que atualizamos os parâmetros da rede baseados somente na previsão para uma amostra. Na prática, nós olhamos mais que uma única amostra. Para fazer isso, nós definimos a chamada <b>função de custo</b>. Essa função de custo nada mais é do que a média dos erros para cada amostra.\n",
    "\n",
    "$$\\mathscr{C} = \\frac{1}{n} \\sum_{i=1}^n \\mathscr{L}(y_i, ŷ_i)$$\n",
    "\n",
    "Se atualizarmos os pesos baseados na função de custo, o gradiente tem a ser mais estável evitando atualizações desnecessárias. A ideia do gradiente descendente é que continuemos a atualizar os pesos até termos um erro satisfatoriamente baixo ou até atingirmos um número de épocas (uma época é uma passada total por todas as amostras da base). Abaixo temos o treino do nosso exemplo para 10 épocas.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_22.gif\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "<h3>Backprogation</h3>\n",
    "\n",
    "Nós discutimos até agora a rede com uma única camada. Mas como calculamos o gradiente quando temos camadas ocultas? Vamos tentar calculá-lo em uma rede bem simples (e que dificilmente será utilizada na prática).\n",
    "\n",
    "<center>\n",
    "<img src=\"images/redes_neurais_23.png\" width=\"800px\" height=\"400px\">\n",
    "</center>\n",
    "\n",
    "Vamos tentar calcular o gradiente de $\\mathscr{L} (y, ŷ)$ (vamos usar a função de perda por simplicidade) em função do parâmetro $w_1$. Mas porque $w_1$ e não $w_2$? Lembre-se que já calculamos $w_2$ quando tratamos a regressão linear acima. Nosso intuito aqui é ver como propagamos o gradiente para a camada oculta.\n",
    "\n",
    "Vamos então escrever as equações que levam a entrada na saída:\n",
    "\n",
    "1. $z_1 = w_1*x + b_1$\n",
    "2. $o_1 = f(z_1)$\n",
    "3. $z_2 = w_2*o_1 + b_2$\n",
    "4. $ŷ = f(z_2)$\n",
    "\n",
    "Da mesma forma que fizemos anteriormente, vamos calculando as derivadas parciais utilizando a regra da cadeia:\n",
    "\n",
    "1. $\\frac{\\partial \\mathscr{L}}{\\partial w_1} = \\frac{\\partial \\mathscr{L}}{\\partial ŷ} \\frac{\\partial ŷ}{\\partial w_1} = (ŷ-y)\\frac{\\partial ŷ}{\\partial w_1}$\n",
    "    \n",
    "2. $\\frac{\\partial ŷ}{\\partial w_1} = \\frac{\\partial ŷ}{\\partial z_2} \\frac{\\partial z_2}{\\partial w_1} = f^\\prime(z_2) \\frac{\\partial z_2}{\\partial w_1}$\n",
    "\n",
    "Até aqui tinhamos já havíamos feito. Vamos agora analisar $z_2$: o termo de $z_2$ que depende de $w_1$ é somente $o_1$. Dessa forma:\n",
    "\n",
    "3. $\\frac{\\partial z_2}{\\partial w_1} = \\frac{\\partial z_2}{\\partial o_1} \\frac{\\partial o_1}{\\partial w_1} = w_2 \\frac{\\partial o_1}{\\partial w_1}$\n",
    "    \n",
    "4. $\\frac{\\partial o_1}{\\partial w_1} = \\frac{\\partial o_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial w_1} = f^\\prime(z_1) \\frac{\\partial o_1}{\\partial w_1}$\n",
    "    \n",
    "5. $\\frac{\\partial z_1}{\\partial w_1} = x$\n",
    "\n",
    "Combinando todas as equações: \n",
    "\n",
    "$$\\frac{\\partial \\mathscr{L}}{\\partial w_1} = (ŷ-y) f^\\prime(z_2) w_2 f^\\prime(z_1) x$$\n",
    "\n",
    "Conseguimos! Obtivemos uma expressão para atualizar $w_1$ mesmo ele estando muito longe da saída. Observe que para fazermos isso, seguimos o sentido contrário da previsão: começamos da saída e chegamos até a entrada. Devido a esse comportamento, de propagação dos gradientes no sentido contrário à previsão, esse algoritmo é conhecido como <b>backpropagation</b>.\n",
    "\n",
    "É isso, pessoal! Espero que tenham gostado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
